{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30474,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Kaggle Start Book in Python \n\n(PythonではじめるKaggleスタートブック)\n\nIshihara, Shotaro; Murata, Hideki Practical Data Science Series: Kaggle Start Book in Python (KS Information Science Specialized Book) . Kodansha.\n\n石原祥太郎; 村田秀樹. 実践Data Scienceシリーズ　PythonではじめるKaggleスタートブック (ＫＳ情報科学専門書) . 講談社. \n\n# section 3.3 Going Beyond Titanic (3)! Let's touch the text data!\n\n(3.3 Titanicの先へ行く③! テキストデータに触れてみよう)","metadata":{}},{"cell_type":"markdown","source":"original NoteBook :\n\nhttps://www.kaggle.com/code/sishihara/py310-python-kaggle-start-book-ch03-03","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2025-05-25T10:46:55.734217Z","iopub.execute_input":"2025-05-25T10:46:55.734863Z","iopub.status.idle":"2025-05-25T10:46:55.776935Z","shell.execute_reply.started":"2025-05-25T10:46:55.734822Z","shell.execute_reply":"2025-05-25T10:46:55.775501Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"df = pd.DataFrame({'text': ['I like kaggle very much',\n                            'I do not like kaggle',\n                            'I do really love machine learning']})\ndf","metadata":{"execution":{"iopub.status.busy":"2025-05-25T10:46:55.778877Z","iopub.execute_input":"2025-05-25T10:46:55.779358Z","iopub.status.idle":"2025-05-25T10:46:55.820191Z","shell.execute_reply.started":"2025-05-25T10:46:55.779315Z","shell.execute_reply":"2025-05-25T10:46:55.818591Z"},"trusted":true},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                text\n0            I like kaggle very much\n1               I do not like kaggle\n2  I do really love machine learning","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I like kaggle very much</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I do not like kaggle</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I do really love machine learning</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# Bag of Words","metadata":{}},{"cell_type":"markdown","source":"How to count the number of times a word appears in a sentence.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n\nvectorizer = CountVectorizer(token_pattern=u'(?u)\\\\b\\\\w+\\\\b')\nbag = vectorizer.fit_transform(df['text'])\nbag.toarray()","metadata":{"execution":{"iopub.status.busy":"2025-05-25T10:46:55.823933Z","iopub.execute_input":"2025-05-25T10:46:55.824648Z","iopub.status.idle":"2025-05-25T10:46:57.382666Z","shell.execute_reply.started":"2025-05-25T10:46:55.824603Z","shell.execute_reply":"2025-05-25T10:46:57.380766Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"array([[0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1],\n       [1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0],\n       [1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0]])"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"print(vectorizer.vocabulary_)","metadata":{"execution":{"iopub.status.busy":"2025-05-25T10:46:57.384454Z","iopub.execute_input":"2025-05-25T10:46:57.384958Z","iopub.status.idle":"2025-05-25T10:46:57.392380Z","shell.execute_reply.started":"2025-05-25T10:46:57.384909Z","shell.execute_reply":"2025-05-25T10:46:57.391041Z"},"trusted":true},"outputs":[{"name":"stdout","text":"{'i': 1, 'like': 4, 'kaggle': 2, 'very': 10, 'much': 7, 'do': 0, 'not': 8, 'really': 9, 'love': 5, 'machine': 6, 'learning': 3}\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":">　 Bag of Words is a simple and easy-to-understand method, but it has the following weaknesses:\n  \n>1. It does not express word rarity. \n>2. It does not take into account the proximity of words \n>3. It leaves out information about the order of words in a sentence.\n\nIshihara, Shotaro; Murata, Hideki Kaggle Start Book in Python (KS Information Science Specialized Book) (p.203). Kodansha. Kindle edition. ","metadata":{}},{"cell_type":"markdown","source":"# TF-IDF","metadata":{}},{"cell_type":"markdown","source":"TF-IDF is a method that takes into account the rarity of the words that appear. It not only counts the “Term Frequency” (the frequency with which a word appears), but also multiplies it by the “Inverse Document Frequency” (the reciprocal of the frequency with which a word appears in a document).","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\n\nvectorizer = CountVectorizer(token_pattern=u'(?u)\\\\b\\\\w+\\\\b')\ntransformer = TfidfTransformer()\n\ntf = vectorizer.fit_transform(df['text'])\ntfidf = transformer.fit_transform(tf)\nprint(tfidf.toarray())","metadata":{"execution":{"iopub.status.busy":"2025-05-25T10:46:57.393590Z","iopub.execute_input":"2025-05-25T10:46:57.394069Z","iopub.status.idle":"2025-05-25T10:46:57.453395Z","shell.execute_reply.started":"2025-05-25T10:46:57.394026Z","shell.execute_reply":"2025-05-25T10:46:57.451192Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[[0.         0.31544415 0.40619178 0.         0.40619178 0.\n  0.         0.53409337 0.         0.         0.53409337]\n [0.43306685 0.33631504 0.43306685 0.         0.43306685 0.\n  0.         0.         0.56943086 0.         0.        ]\n [0.34261996 0.26607496 0.         0.45050407 0.         0.45050407\n  0.45050407 0.         0.         0.45050407 0.        ]]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(vectorizer.vocabulary_)","metadata":{"execution":{"iopub.status.busy":"2025-05-25T10:46:57.454582Z","iopub.execute_input":"2025-05-25T10:46:57.455100Z","iopub.status.idle":"2025-05-25T10:46:57.471516Z","shell.execute_reply.started":"2025-05-25T10:46:57.455054Z","shell.execute_reply":"2025-05-25T10:46:57.469646Z"},"trusted":true},"outputs":[{"name":"stdout","text":"{'i': 1, 'like': 4, 'kaggle': 2, 'very': 10, 'much': 7, 'do': 0, 'not': 8, 'really': 9, 'love': 5, 'machine': 6, 'learning': 3}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Word2vec","metadata":{}},{"cell_type":"markdown","source":"A Vectoring Method for Capturing the Proximity of Word Meanings","metadata":{}},{"cell_type":"code","source":"from gensim.models import word2vec\n\n\nsentences = [d.split() for d in df['text']]\nmodel = word2vec.Word2Vec(sentences, vector_size=10, min_count=1, window=2, seed=7)","metadata":{"execution":{"iopub.status.busy":"2025-05-25T10:46:57.472927Z","iopub.execute_input":"2025-05-25T10:46:57.473370Z","iopub.status.idle":"2025-05-25T10:46:57.789250Z","shell.execute_reply.started":"2025-05-25T10:46:57.473334Z","shell.execute_reply":"2025-05-25T10:46:57.787752Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"model.wv['like']","metadata":{"execution":{"iopub.status.busy":"2025-05-25T10:46:57.790676Z","iopub.execute_input":"2025-05-25T10:46:57.791078Z","iopub.status.idle":"2025-05-25T10:46:57.799533Z","shell.execute_reply.started":"2025-05-25T10:46:57.791046Z","shell.execute_reply":"2025-05-25T10:46:57.798092Z"},"trusted":true},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"array([ 0.01650858,  0.01069946,  0.00188946,  0.09910005,  0.06153275,\n        0.05853238,  0.04005488,  0.02443584, -0.03179482,  0.09779203],\n      dtype=float32)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"model.wv.most_similar('like')","metadata":{"execution":{"iopub.status.busy":"2025-05-25T10:46:57.802761Z","iopub.execute_input":"2025-05-25T10:46:57.803215Z","iopub.status.idle":"2025-05-25T10:46:57.815464Z","shell.execute_reply.started":"2025-05-25T10:46:57.803181Z","shell.execute_reply":"2025-05-25T10:46:57.814285Z"},"trusted":true},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"[('I', 0.42540043592453003),\n ('machine', 0.36355969309806824),\n ('not', 0.311229407787323),\n ('kaggle', -0.004140517208725214),\n ('much', -0.11530755460262299),\n ('do', -0.1529018133878708),\n ('love', -0.25542783737182617),\n ('really', -0.4161785840988159),\n ('learning', -0.44330504536628723),\n ('very', -0.44338396191596985)]"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"df['text'][0].split()","metadata":{"execution":{"iopub.status.busy":"2025-05-25T10:46:57.817200Z","iopub.execute_input":"2025-05-25T10:46:57.817615Z","iopub.status.idle":"2025-05-25T10:46:57.831514Z","shell.execute_reply.started":"2025-05-25T10:46:57.817570Z","shell.execute_reply":"2025-05-25T10:46:57.829799Z"},"trusted":true},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"['I', 'like', 'kaggle', 'very', 'much']"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"import numpy as np\n\n\nwordvec = np.array([model.wv[word] for word in df['text'][0].split()])\nwordvec","metadata":{"execution":{"iopub.status.busy":"2025-05-25T10:46:57.833152Z","iopub.execute_input":"2025-05-25T10:46:57.833618Z","iopub.status.idle":"2025-05-25T10:46:57.849107Z","shell.execute_reply.started":"2025-05-25T10:46:57.833580Z","shell.execute_reply":"2025-05-25T10:46:57.847773Z"},"trusted":true},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"array([[ 0.08898099,  0.02501909,  0.03683598,  0.07944275,  0.01565849,\n         0.05513714,  0.0667302 , -0.05495857, -0.08889369, -0.03996675],\n       [ 0.01650858,  0.01069946,  0.00188946,  0.09910005,  0.06153275,\n         0.05853238,  0.04005488,  0.02443584, -0.03179482,  0.09779203],\n       [ 0.06329302, -0.03939352, -0.03167932, -0.04431488,  0.04389417,\n        -0.04902608,  0.09809195, -0.01098474, -0.00437022,  0.00090965],\n       [ 0.03720424, -0.02774719,  0.02864924,  0.01963681, -0.07835456,\n        -0.08814968,  0.03203132, -0.02247364,  0.01966591, -0.03539274],\n       [-0.09157717,  0.04835419, -0.00529734, -0.08170088, -0.05110302,\n         0.00822875,  0.04535742,  0.00155444,  0.02258943,  0.07426786]],\n      dtype=float32)"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"> By vectorizing words with this idea, sentences can be used as input for machine learning algorithms. The following are some examples of such methods.\n\n> 1. Take the average of the vectors of words appearing in a sentence\n> 2. Take the maximum value of each element of the word vectors appearing in a sentence\n> 3. Treat each word as time series data\n\nIshihara, Shotaro; Murata, Hideki Kaggle Start Book in Python (KS Information Science Specialized Book) (p.205). Kodansha. Kindle edition. ","metadata":{}},{"cell_type":"markdown","source":"### 1. Take the average of the vectors of words appearing in a sentence","metadata":{}},{"cell_type":"code","source":"np.mean(wordvec, axis=0)","metadata":{"execution":{"iopub.status.busy":"2025-05-25T10:46:57.850751Z","iopub.execute_input":"2025-05-25T10:46:57.851857Z","iopub.status.idle":"2025-05-25T10:46:57.865527Z","shell.execute_reply.started":"2025-05-25T10:46:57.851810Z","shell.execute_reply":"2025-05-25T10:46:57.864130Z"},"trusted":true},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"array([ 0.02288193,  0.00338641,  0.0060796 ,  0.01443277, -0.00167443,\n       -0.0030555 ,  0.05645315, -0.01248533, -0.01656068,  0.01952201],\n      dtype=float32)"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"### 2. Take the maximum value of each element of the word vectors appearing in a sentence","metadata":{}},{"cell_type":"code","source":"np.max(wordvec, axis=0)","metadata":{"execution":{"iopub.status.busy":"2025-05-25T10:46:57.867061Z","iopub.execute_input":"2025-05-25T10:46:57.867518Z","iopub.status.idle":"2025-05-25T10:46:57.886954Z","shell.execute_reply.started":"2025-05-25T10:46:57.867471Z","shell.execute_reply":"2025-05-25T10:46:57.885351Z"},"trusted":true},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"array([0.08898099, 0.04835419, 0.03683598, 0.09910005, 0.06153275,\n       0.05853238, 0.09809195, 0.02443584, 0.02258943, 0.09779203],\n      dtype=float32)"},"metadata":{}}],"execution_count":13}]}